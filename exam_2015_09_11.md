**Exercise 1**

1.1

Short answer: no.
Here you can draw the precedence graph and see that transactions 2 and 4 have a cycle thus, since every legal 2PL schedule with shared and exclusive locks is conflict-serializable we can conclude that the schedule is not 2PL.

1.2

We already saw that the schedule is not conflict-serializable. We compute the READ-FROM and FINAL-WRITE sets:

 1. READ-FROM: {(3,1), (3,2), (3,2)}
 2. FINAL-WRITE: {w2(Z), w4(v), w2(x), w3(y)}

The schedule cannot be view-serializable: while it's clear that the T3 must come before T1 and T2, the problem is that T4 must come after T3 and T2 so that the write on V doesn't cause a new READ-FROM relation. But since T4 and T2 both write on Z this would cause Z to have a different FINAL-WRITE operation in S'.

1.3

To answer this question we proceed backwards:

(*iv*) Is it rigorous? No, it is simple to see that since a schedule S is rigorous if for each pair of conflicting actions ai and bj the commit command ci of Ti appears in S between ai and bj, then w3 should commit after the first write on X, but that's not the possibile because there are also other actions pending on T3 ( w3(Z) and r3(V) ).

(*iii*) Is it strict? A schedule is strict if no transactions read from transactions that have not commited, and no transactions write on transaction that have not commit yet. Thus, since the schedule r1(x) reads from w3(x) and T3 has not committed yet then we can conclude that the schedule is not strict.

(*ii*) Is it ACR? No because an ACR schedule is a schedule in which a transaction cannot read from another transaction before it has commited. The example above iilustrates why S is not ACR.

(*i*) Is it recoverable? T1 commits before T3 while reading from it, thus it is not recoverable!

***
**Exercise 3**

An index is **dense** if every element of the search key in the realation appears in the data entries of the index.
An index is **clustering** if the tuples of the relation and the data entries of the index are ordered on the same search key.
An index is **sorted** if the data entries of the index are sorted on a given attribute.
An index is **primary** if the search key of the index includes the primary key of the relation.

Given a value K of the search key, one can perform a binary search on the index file on the value K and then follow the pointer to the page file containing the data record with search key K. In fact, index file contains the search key and pointers to the relative data record. Assuming that B is the number of index pages, given a dense clustering sorted primary index one can find a value K in 

log2(B) + 1 page accesses.

The algorithm is very trivial:

we want to search K in the range of page addresses (h1, h2). If the range is empty then stop with failure, else choose a tentative page address i (h1 &lt;= i &lt;= h2) and load page pi at address i. If K is in the page pi then stop with success, else if K is less than the minimum key value of the page repeat the algorithm using the rage (h1, pi-1) else repeat the algorithm using (pi+1, h2). In binary search the tentative address is always the one at the half of range.
***
**Exercise 4**

20B per attributes * 5 attributes  = 100B per data record.
1000B per page / 100B per data record = 10 data records per page.
10'000'000 data records / 10 data records per page = 1'000'000 pages.

The query are selection on attribute and range selection, we don't have any information about the insertion and deletion of tuples in the system, assuming there are many we can use a B+-tree index with alternative 2 which is perfectly suited for the task. We also don't have any information about wether the tree is clustering or not.

With that said we will have an index file in which each data record has 2 field: code and a pointer to the index page. We will study both cases of dense and sparse index assuming that the data file is clustering on the attribute code.

_In case of a dense file we would have:_
20B per attribtues * 2 attributes = 40B per data entry.
1000B per page / 40B per data entry = 25 data records per page
67% occupancy rule ==&gt; 25*0.67 = 17 data records per page.

10'000'000 / 17 = 588'236 index pages.

Fan-out is (10 + 25) / 2 = 18.

Cost of searching is

logF(I) + 1 =  log18(588'236) + 1 = 5 + 1 = 6 page accesses.

_In case of a sparse file we would have:_

1'000'000 pages / 17 = 58'824 pages (one per data page)

Cost of searching 

logF(I) + 1 = 4 + 1 = 5 page accesses

So a sparse index seems more suitable for the task.

The first query doesn't need the usage of the index since it is a scan operation and the cost of the scan is B where B is the number of pages of the relation. Since we assumed a clustered data file ordered on attribute code, the order by is naturally performed by the scan that scans the tuples on an ordered fashion. 

The range query costs is:

logF(I) + 1 = log18(58'824) + furtherPages = 4 + furtherPages

Since **code** is a key of the relation we know that there are 20 tuples from 30 to 50. This means that at least we need to load 2 more pages in order to find every value from 30 to 50. So the cost is

logF(I) + 1 = log18(58'824) + furtherPages = 4 + 2 = 6 page accesses.
***
**Exercise 5**

5.1 

The one pass algorithm works as follow: load each page of the relation R, and for each tuple of the page check if the record is in a data structure containing all the occurrence of that tuple (the data structure can be a hash map for efficient access). If the tuple is present in the data structure then do nothing, otherwise add it to the data structure and copy it in the output frame.

The cost of the algorithm is B where B is the number of pages of the relation.

5.2

The two-pass algorithm makes use of sorting. In order to use the two-pass algorithm we must ensure that

B &lt;= M(M-1) where B is the number of pages of the relation and M is the number of buffer frames.

If the condition is satisfied we can proceed to sort the relation in the pass 0 using one of the method we already know (two-way merge-sort or multi-way merge-sort). Once that the relation is sorted we use the available frames to hold one page for each sublist. For each element in the buffer frame, copy in the output and remove it from the sublists. Do it for each page of the sublists.