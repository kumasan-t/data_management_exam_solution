**Exercise 1**
The fix operation is a read operation performed by the Buffer Manager. Each frame of the Buffer Manager has associated the information bout which page it cointains, the *pin-count* which is a variable that indicates how many transactions are using that page, and a *dirty* bit whose value indicates if a page has been modified (true) or not (false). The buffer manager that want to load a page from secondary storage must first choose a buffer frame to replace with the new page.
Several policies are used to perform this such as a LRU policty (*Least Recently Used*) which replace only pages whose pin-count is equal to 0. 
A more sophisticated approach is through a *Clock Replacement* policy:
we use a variable called to current that scan the buffer frames in a circular fashion. Every frame that is used recently sets the bit *referenced* to 1 (initially set to zero for all buffer frames). When we look for a replacement we look at the frame pointed by current:

 - if pin-count &gt; 0 then we move current on the next frame
 - if pint-count = 0 and reference = true we set reference to false to reflect the fact that time has passed from the last usage of the frame, we then increment current and we repeat the procedure.
 - if pint-count = 0 and reference = false we select that page to be replaced.

***
**Exercise 2**
2.1 Can we use block-nested loop algorithm?
The only requirement is to have a buffer of M &gt; 2 free frames. We have 122, so surely we can use a block nested loop algorithm. The cost is
5000 + 5000 * 8000 / (122 - 2) = 338'334

2.2 We can also use a two pass algorithm in the 2-way variant since B(R) + B(S) &lt;= 121*121
The algorithm simply sort the two relation on the attribute C and D (the join attribute) and computes the join in this way:

 1. Pass 0: reserve an amount of buffer frame for R and S such that the number of sublists produced by pass 0 is equal to M-1. Say 61 frames for R and 60 frames for S. This will produce 61 sublists for R and 60 sublists for S.
 2. Pass 1: we show that indeed we don't need that much frames since the number of tuples of R having the same value is 8000 * 100 / 200 = 4000 tuples which can be stored on 40 frames and the number of tuples of S having the same value is 5000 * 100 / 160 = 3125 tuples which can be stored on 32 frames. This means that if we load 40 sublists of R and 32 sublists of S we are sure that we can compute the merge for a given value of C (or D). The cost of the algorithm is 3 * (5000 + 8000) = 75'000 page accesses.
***
**Exercise 3**
3.1 It is not conflict serializable: the precedence graph associated to S shows a cycle between T1, T5 and T2.
3.2 The minimal number of transactions that must be deleted from S so that the resulting scheme is conflict-serializable is 1. We may simply ignore transaction T5.
3.3 To check if S is view-serializable we must build the sets of READS-FROM and FINAL-WRITE:

 1. READS-FROM={(4,1),(5,1)} 
 2. FINAL-WRITE={W5(y), W5(v), W2(x), W3(z)}

This indicates that T1 must come after T4 and T5 but before T2 and T3. However T3 must come before T2 and T4, but if that's the case then we would have a different READS-FROM set such that the new schedule S' is not view-equivalent to S. 
3.4 Since all reads happen on transactions that have already committed we can conclude that the schedule is indeed ACR.
***
**Exercise 5**
4'000'000 / 400'000 = 10 tuples per page. 4'000'000 / 10'000 = 400 tuples per cost. 
10 * 4 = x * 2 =&gt; x = 20 tuples per page in the ISAM tree.
400 / 10 = 40 pages of tuples for each different value of cost.
67% occupancy rule, 20 * 0.67 = 13,42 == 14 tuples per page, 400'000 / 14 = 28'572 pages in the ISAM tree.
The text suggests that alternative 2 is used with a sparse index that by definition is clustered. Suppose we have a fan-out of 15 ((10+20)/2). The number of page accesses of the range query on 20 values is LogF (28'572) + 20 * 40 = 2 + 800 = 802