**Exercise 1**
***
**Exercise 2**

2.1

xl3(y) w3(y) sl3(t) r3(t) sl1(x) sl1(z) r1(x) r1(z) r3(z) sl3(z) r3(z) u3(y) u3(t) u3(z) xl1(y) u1(y) u1(z) xl2(x)  w2(x) u2(x) w2(y) u2(y) sl4(x) r4(x) u4(x).

The schedule is 2PL with shared and exclusive locks.

2.3

w3(y) r3(t) r1(x) r1(z) w2(x) c2 r3(z) c3 w1(y) c1 r4(x) c4.

The schedule is a 2PL strict protocol since no transaction reads from a transaction that has not committed and no transactions writes on another transaction before it has committed.

2.2

It is not strong strict because T1 must release the lock on x to allow the write on x by T2.
***
**Exercise 3**

 - w3(y) OK ; wts(y)=3 ; cb(y)=false;
 - r3(t) OK ; rts(t)=3 ;
 - r1(x) OK ; rts(x)=1 ;
 - w1(z) OK ; wts(z)=1 ; cb(z)=false;
 - c1 OK ; wts-c(z)=1 ; cb(z)=true;
 - w2(x) OK ; wts(x)=2 ; cb(x)=false;
 - r3(z) OK ; rts(z)=3 ;
 - c3 OK ; cb(y)=true;
 - w2(y) Thomas Rule, we ignore the update.
 - c2 OK
 - r4(x) OK ; rts(x)=4;
 - c4 OK;
 
***
**Exercise 4**

In this case we can use a multipass merge-sort algorithm in which Z sublists are produced where Z = 99 frames and then merged back together.

The algorithm procedes as follow: load 99 pages onto 99 free buffer frames and copy in the output the tuples ordered in some fashion, producing a so called **run**. The first step produces S=B/Z runs. Now pick those S runs and sort them in the same way, producing S/Z runs. In the next step S/Z runs are sorted and merged in S/Z/Z runs which is S/Z^2 runs.This will produce k steps of:

S/Z, S/Z^2, S/Z^3, ... , S/Z^k runs

The merge-sorting of the run stops when S/Z^k = 1 ==> S = Z^k ==> k = logZ(S) ==> logZ(B/Z) ==> logZ(B) - 1.

The total cost of the algorithm is 2 x B x (logZ(B) + 1).

In this particular case the cost is 2 x 100'000 x log99(100'000) = 600'000 page accesses.
***
**Exercise 5**

Using a B+ tree gives no improvement in terms of space occupancy since a tree-index on the attribute of Person would be composed of 3 attributes (last name, first name, age) + 1 pointer to the data record i.e. 4 attributes (same as the relation Person). A sorted file seems more efficient, even though using an index-tree reduces the number of space accesses.

30B per attribute * 4 = 120B per tuple.
1200B / 120B = 10 tuples per page.
1'000'000 / 10 = 100'000 pages needed to store the relation.